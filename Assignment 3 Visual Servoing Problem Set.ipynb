{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95e3ae5",
   "metadata": {},
   "source": [
    "# Assignment 5: Visual Servoing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ec1fb",
   "metadata": {},
   "source": [
    "The following has ALL the code to run the simulation and all the problems. You do not need to understand exactly how it works. You only need to work on the sections starting at \"COMPLETE THIS\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaab71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import os    \n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "from scipy.spatial.transform import Rotation as Rot            #can use this to apply angular rotations to coordinate frames\n",
    "\n",
    "#camera (don't change these settings)\n",
    "camera_width = 512                                             #image width\n",
    "camera_height = 512                                            #image height\n",
    "camera_fov = 120                                                #field of view of camera\n",
    "camera_focal_depth = 0.5*camera_height/np.tan(0.5*np.pi/180*camera_fov) \n",
    "                                                               #focal depth in pixel space\n",
    "camera_aspect = camera_width/camera_height                     #aspect ratio\n",
    "camera_near = 0.02                                             #near clipping plane in meters, do not set non-zero\n",
    "camera_far = 100                                               #far clipping plane in meters\n",
    "\n",
    "\n",
    "#control objectives (if you wish, you can play with these values for fun)\n",
    "object_location_desired = np.array([camera_width/2,camera_height/2])\n",
    "                                                               #center the object to middle of image \n",
    "K_p_x = 0.1                                                    #Proportional control gain for translation\n",
    "K_p_Omega = 0.02                                               #Proportional control gain for rotation       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c20c1b",
   "metadata": {},
   "source": [
    "### Create the Robot Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd2cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robot with Camera Class\n",
    "class eye_in_hand_robot:\n",
    "    def get_ee_position(self):\n",
    "        '''\n",
    "        Function to return the end-effector of the link. This is the very tip of the robot at the end of the jaws.\n",
    "        '''\n",
    "        endEffectorIndex = self.numActiveJoints\n",
    "        endEffectorState = p.getLinkState(self.robot_id, endEffectorIndex)\n",
    "        endEffectorPos = np.array(endEffectorState[0])\n",
    "        endEffectorOrn = np.array(p.getMatrixFromQuaternion(endEffectorState[1])).reshape(3,3)\n",
    "        \n",
    "        #add an offset to get past the forceps\n",
    "        endEffectorPos += self.camera_offset*endEffectorOrn[:,2]\n",
    "        return endEffectorPos, endEffectorOrn\n",
    "\n",
    "    def get_current_joint_angles(self):\n",
    "        # Get the current joint angles\n",
    "        joint_angles = np.zeros(self.numActiveJoints)\n",
    "        for i in range(self.numActiveJoints):\n",
    "            joint_state = p.getJointState(self.robot_id, self._active_joint_indices[i])\n",
    "            joint_angles[i] = joint_state[0]\n",
    "        return joint_angles\n",
    "    \n",
    "    def get_jacobian_at_current_position(self):\n",
    "        #Returns the Robot Jacobian of the last active link\n",
    "        mpos, mvel, mtorq = self.get_active_joint_states()   \n",
    "        zero_vec = [0.0]*len(mpos)\n",
    "        linearJacobian, angularJacobian = p.calculateJacobian(self.robot_id, \n",
    "                                                              self.numActiveJoints,\n",
    "                                                              [0,0,self.camera_offset],\n",
    "                                                              mpos, \n",
    "                                                              zero_vec,\n",
    "                                                              zero_vec)\n",
    "        #only return the active joint's jacobians\n",
    "        Jacobian = np.vstack((linearJacobian,angularJacobian))\n",
    "        return Jacobian[:,:self.numActiveJoints]\n",
    "    \n",
    "    def set_joint_position(self, desireJointPositions, kp=1.0, kv=0.3):\n",
    "        '''Set  the joint angle positions of the robot'''\n",
    "        zero_vec = [0.0] * self._numLinkJoints\n",
    "        allJointPositionObjectives = [0.0]*self._numLinkJoints\n",
    "        for i in range(desireJointPositions.shape[0]):\n",
    "            idx = self._active_joint_indices[i]\n",
    "            allJointPositionObjectives[idx] = desireJointPositions[i]\n",
    "\n",
    "        p.setJointMotorControlArray(self.robot_id,\n",
    "                                    range(self._numLinkJoints),\n",
    "                                    p.POSITION_CONTROL,\n",
    "                                    targetPositions=allJointPositionObjectives,\n",
    "                                    targetVelocities=zero_vec,\n",
    "                                    positionGains=[kp] * self._numLinkJoints,\n",
    "                                    velocityGains=[kv] * self._numLinkJoints)\n",
    "\n",
    "    def get_active_joint_states(self):\n",
    "        '''Get the states (position, velocity, and torques) of the active joints of the robot\n",
    "        '''\n",
    "        joint_states = p.getJointStates(self.robot_id, range(self._numLinkJoints))\n",
    "        joint_infos = [p.getJointInfo(self.robot_id, i) for i in range(self._numLinkJoints)]\n",
    "        joint_states = [j for j, i in zip(joint_states, joint_infos) if i[3] > -1]\n",
    "        joint_positions = [state[0] for state in joint_states]\n",
    "        joint_velocities = [state[1] for state in joint_states]\n",
    "        joint_torques = [state[3] for state in joint_states]\n",
    "        return joint_positions, joint_velocities, joint_torques\n",
    "\n",
    "\n",
    "         \n",
    "    def __init__(self, robot_id, initialJointPos):\n",
    "        self.robot_id = robot_id\n",
    "        self.eeFrameId = []\n",
    "        self.camera_offset = 0.1 #offset camera in z direction to avoid grippers\n",
    "        # Get the joint info\n",
    "        self._numLinkJoints = p.getNumJoints(self.robot_id) #includes passive joint\n",
    "        jointInfo = [p.getJointInfo(self.robot_id, i) for i in range(self._numLinkJoints)]\n",
    "        \n",
    "        # Get joint locations (some joints are passive)\n",
    "        self._active_joint_indices = []\n",
    "        for i in range(self._numLinkJoints):\n",
    "            if jointInfo[i][2]==p.JOINT_REVOLUTE:\n",
    "                self._active_joint_indices.append(jointInfo[i][0])\n",
    "        self.numActiveJoints = len(self._active_joint_indices) #exact number of active joints\n",
    "\n",
    "        #reset joints\n",
    "        for i in range(self._numLinkJoints):\n",
    "            p.resetJointState(self.robot_id,i,initialJointPos[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7ae124",
   "metadata": {},
   "source": [
    "### Ancilliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54ddbb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_coordinate_frame(position, orientation, length, frameId = []):\n",
    "    '''\n",
    "    Draws a coordinate frame x,y,z with scaled lengths on the axes \n",
    "    in a position and orientation relative to the world coordinate frame\n",
    "    pos: 3-element numpy array\n",
    "    orientation: 3x3 numpy matrix\n",
    "    length: length of the plotted x,y,z axes\n",
    "    frameId: a unique ID for the frame. If this supplied, then it will erase the previous location of the frame\n",
    "    \n",
    "    returns the frameId\n",
    "    '''\n",
    "    if len(frameId)!=0:\n",
    "        p.removeUserDebugItem(frameId[0])\n",
    "        p.removeUserDebugItem(frameId[1])\n",
    "        p.removeUserDebugItem(frameId[2])\n",
    "    \n",
    "    lineIdx=p.addUserDebugLine(position, position + np.dot(orientation, [length, 0, 0]), [1, 0, 0])  # x-axis in red\n",
    "    lineIdy=p.addUserDebugLine(position, position + np.dot(orientation, [0, length, 0]), [0, 1, 0])  # y-axis in green\n",
    "    lineIdz=p.addUserDebugLine(position, position + np.dot(orientation, [0, 0, length]), [0, 0, 1])  # z-axis in blue\n",
    "\n",
    "    return lineIdx,lineIdy,lineIdz\n",
    "\n",
    "def opengl_plot_world_to_pixelspace(pt_in_3D_to_project, viewMat, projMat, imgWidth, imgHeight):\n",
    "    ''' Plots a x,y,z location in the world in an openCV image\n",
    "    This is used for debugging, e.g. given a known location in the world, verify it appears in the camera\n",
    "    when using p.getCameraImage(...). The output [u,v], when plot with opencv, should line up with object \n",
    "    in the image from p.getCameraImage(...)\n",
    "    '''\n",
    "    pt_in_3D_to_project = np.append(pt_in_3D_to_project,1)\n",
    "    #print('Point in 3D to project:', pt_in_3D_to_project)\n",
    "\n",
    "    pt_in_3D_in_camera_frame = viewMat @ pt_in_3D_to_project\n",
    "    #print('Point in camera space: ', pt_in_3D_in_camera_frame)\n",
    "\n",
    "    # Convert coordinates to get normalized device coordinates (before rescale)\n",
    "    uvzw = projMat @ pt_in_3D_in_camera_frame\n",
    "    #print('after projection: ', uvzw)\n",
    "\n",
    "    # scale to get the normalized device coordinates\n",
    "    uvzw_NDC = uvzw/uvzw[3]\n",
    "    #print('after normalization: ', uvzw_NDC)\n",
    "\n",
    "    #x,y specifies lower left corner of viewport rectangle, in pixels. initial value is (0,)\n",
    "    u = ((uvzw_NDC[0] + 1) / 2.0) * imgWidth\n",
    "    v = ((1-uvzw_NDC[1]) / 2.0) * imgHeight\n",
    "\n",
    "    return [int(u),int(v)]\n",
    "\n",
    "    \n",
    "def get_camera_view_and_projection_opencv(cameraPos, camereaOrn):\n",
    "    '''Gets the view and projection matrix for a camera at position (3) and orientation (3x3)'''\n",
    "    __camera_view_matrix_opengl = p.computeViewMatrix(cameraEyePosition=cameraPos,\n",
    "                                                   cameraTargetPosition=cameraPos+camereaOrn[:,2],\n",
    "                                                   cameraUpVector=-camereaOrn[:,1])\n",
    "\n",
    "    __camera_projection_matrix_opengl = p.computeProjectionMatrixFOV(camera_fov, camera_aspect, camera_near, camera_far)        \n",
    "    _, _, rgbImg, depthImg, _ = p.getCameraImage(camera_width, \n",
    "                                                 camera_height, \n",
    "                                                 __camera_view_matrix_opengl,\n",
    "                                                 __camera_projection_matrix_opengl, \n",
    "                                                 renderer=p.ER_BULLET_HARDWARE_OPENGL)\n",
    "\n",
    "    #returns camera view and projection matrices in a form that fits openCV\n",
    "    viewMat = np.array(__camera_view_matrix_opengl).reshape(4,4).T\n",
    "    projMat = np.array(__camera_projection_matrix_opengl).reshape(4,4).T\n",
    "    return viewMat, projMat\n",
    "\n",
    "def get_camera_img_float(cameraPos, camereaOrn):\n",
    "    ''' Gets the image and depth map from a camera at a position cameraPos (3) and cameraOrn (3x3) in space. '''\n",
    "    __camera_view_matrix_opengl = p.computeViewMatrix(cameraEyePosition=cameraPos,\n",
    "                                                   cameraTargetPosition=cameraPos+camereaOrn[:,2],\n",
    "                                                   cameraUpVector=-camereaOrn[:,1])\n",
    "\n",
    "    __camera_projection_matrix_opengl = p.computeProjectionMatrixFOV(camera_fov, camera_aspect, camera_near, camera_far)        \n",
    "    width, height, rgbImg, nonlinDepthImg, _ = p.getCameraImage(camera_width, \n",
    "                                                 camera_height, \n",
    "                                                 __camera_view_matrix_opengl,\n",
    "                                                 __camera_projection_matrix_opengl, \n",
    "                                                 renderer=p.ER_BULLET_HARDWARE_OPENGL)\n",
    "\n",
    "    #adjust for clipping and nonlinear distance i.e., 1/d (0 is closest, i.e., near, 1 is furthest away, i.e., far\n",
    "    depthImgLinearized =camera_far*camera_near/(camera_far+camera_near-(camera_far-camera_near)*nonlinDepthImg)\n",
    "\n",
    "    #convert to numpy and a rgb-d image\n",
    "    rgb_image = np.array(rgbImg[:,:,:3], dtype=np.uint8)\n",
    "    depth_image = np.array(depthImgLinearized, dtype=np.float32)\n",
    "    return rgb_image, depth_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dd45eb",
   "metadata": {},
   "source": [
    "### Create Physics Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc1d832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the connection to the physics server\n",
    "physicsClient = p.connect(p.GUI)#(p.DIRECT)\n",
    "time_step = 0.001\n",
    "p.resetSimulation()\n",
    "p.setTimeStep(time_step)\n",
    "p.setGravity(0, 0, -9.8)\n",
    "\n",
    "# Set the path to the URDF files included with PyBullet\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "\n",
    "# load a plane URDF\n",
    "p.loadURDF('plane.urdf')\n",
    "\n",
    "# Place obstacles\n",
    "box_length = box_width = 0.4\n",
    "box_depth = 0.02\n",
    "object_center = [0.3, 1, 0.01]\n",
    "object_orientation = [0, 0, 0]\n",
    "object_color = [0.8, 0.0, 0.0, 1]\n",
    "geomBox = p.createCollisionShape(p.GEOM_BOX, halfExtents=[box_length/2, box_width/2, box_depth/2])\n",
    "visualBox = p.createVisualShape(p.GEOM_BOX, halfExtents=[box_length/2, box_width/2, box_depth/2], rgbaColor=object_color)\n",
    "boxId = p.createMultiBody(\n",
    "    baseMass=0,\n",
    "    baseCollisionShapeIndex=geomBox,\n",
    "    baseVisualShapeIndex=visualBox,\n",
    "    basePosition=np.array(object_center),\n",
    "    baseOrientation=p.getQuaternionFromEuler(object_orientation)\n",
    ")\n",
    "ObjectModelPos, modelOrn  = p.getBasePositionAndOrientation(boxId)\n",
    "\n",
    "#reset debug gui camera position so we can see the robot up close\n",
    "p.resetDebugVisualizerCamera( cameraDistance=1, cameraYaw=30, cameraPitch=-52, cameraTargetPosition=[0,0,.5])\n",
    "p.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5bb297",
   "metadata": {},
   "source": [
    "### COMPLETE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e1ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q1: Show movement in Jacobian\n",
    "def getImageJacobian(u_px,v_px,depthImg,focal_length, imgWidth, imgHeight):\n",
    "    ''' Inputs: \n",
    "    u_px, v_px is pixel coordinates from image with top-left corner as 0,0\n",
    "    depthImg is the depth map\n",
    "    f is the focal length\n",
    "    \n",
    "    Outputs: image_jacobian, a 2x6 matrix'''\n",
    "\n",
    "\n",
    "    # Convert to image-centered coordinates\n",
    "    u = u_px - imgWidth / 2.0\n",
    "    v = imgHeight / 2.0 - v_px   # invert y so +v is up\n",
    "    \n",
    "    # Get depth at pixel\n",
    "    Z = depthImg[int(v_px), int(u_px)]\n",
    "    \n",
    "    f = focal_length\n",
    "    image_jacobian = np.zeros((2,6))\n",
    "    \n",
    "    # Fill in entries\n",
    "    image_jacobian[0,0] = -f / Z\n",
    "    image_jacobian[0,1] = 0\n",
    "    image_jacobian[0,2] = u / Z\n",
    "    image_jacobian[0,3] = (u * v) / f\n",
    "    image_jacobian[0,4] = -(f**2 + u**2) / f\n",
    "    image_jacobian[0,5] = v\n",
    "    \n",
    "    image_jacobian[1,0] = 0\n",
    "    image_jacobian[1,1] = -f / Z\n",
    "    image_jacobian[1,2] = v / Z\n",
    "    image_jacobian[1,3] = (f**2 + v**2) / f\n",
    "    image_jacobian[1,4] = -(u * v) / f\n",
    "    image_jacobian[1,5] = -u\n",
    "\n",
    "\n",
    "    return image_jacobian\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "664f59d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCameraControl(object_loc_des, object_loc, image_jacobian):\n",
    "    ''' Inputs:\n",
    "    object_loc_des: desired [x,y] pixel locations for object\n",
    "    object_loc: current [x,y] pixel locations as found from computer vision\n",
    "    image_jacobian: the image jacobian \n",
    "        Outputs:\n",
    "    delta_X: the scaled displacement in position of camera (world frame) to reduce the error\n",
    "    delta_Omega: the scaled angular velocity of camera (world frame omega-x,y,z) to reduce the error\n",
    "    \n",
    "    '''\n",
    "    lam = 0.5\n",
    "    # Compute image error (2x1)\n",
    "    e = np.array(object_loc) - np.array(object_loc_des)   # [u - u*, v - v*]\n",
    "    \n",
    "    # Compute pseudoinverse of image Jacobian (6x2)\n",
    "    L_pinv = np.linalg.pinv(image_jacobian)\n",
    "    \n",
    "    # Compute camera twist (6x1)\n",
    "    v_c = -lam * L_pinv @ e     # control law\n",
    "    \n",
    "    # Split into translation and rotation\n",
    "    delta_X = v_c[0:3]          # translational velocity\n",
    "    delta_Omega = v_c[3:6]      # angular velocity\n",
    "    \n",
    "    return delta_X, delta_Omega\n",
    "    \n",
    "    # return delta_X = [], delta_Omega "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2b2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     return new_jointPositions = []\n",
    "\n",
    "\n",
    "def findJointControl(robot, delta_X, delta_Omega):\n",
    "    ''' Inputs:\n",
    "    delta_X: the scaled displacement in position of camera (world frame) to reduce the error\n",
    "    delta_Omega: the scaled angular velocity of camera (world frame omega-x,y,z) to reduce the error\n",
    "    Outputs:\n",
    "    delta_Q: the change in robot joints to cause the camera to move delta_X and delta_Omega\n",
    "    '''\n",
    "\n",
    "    # Combine translational and rotational components into a single 6x1 twist\n",
    "    v_c = np.hstack((delta_X, delta_Omega))  # [vx, vy, vz, wx, wy, wz]\n",
    "    \n",
    "    # Get current joint angles\n",
    "    q_current = np.array(robot.get_current_joint_angles())\n",
    "    \n",
    "    # Get current geometric Jacobian (6 x n)\n",
    "    J = robot.get_jacobian_at_current_position()\n",
    "    \n",
    "    # Compute pseudoinverse of Jacobian\n",
    "    J_pinv = np.linalg.pinv(J)\n",
    "    \n",
    "    # Compute joint velocity command (change in joint space)\n",
    "    delta_Q = J_pinv @ v_c    # (n,)\n",
    "    \n",
    "    # Compute new joint positions\n",
    "    new_jointPositions = q_current + delta_Q\n",
    "\n",
    "    return new_jointPositions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c41de24",
   "metadata": {},
   "source": [
    "## SIMULATE ANSWERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fac3dc4",
   "metadata": {},
   "source": [
    "### Q1 and Q2 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68092c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Recording significant motion: No_Motion ===\n",
      "Saved No_Motion_motion.gif\n",
      "\n",
      "=== Recording significant motion: Trans_X+ ===\n",
      "Saved Trans_X+_motion.gif\n",
      "\n",
      "=== Recording significant motion: Trans_Y+ ===\n",
      "Saved Trans_Y+_motion.gif\n",
      "\n",
      "=== Recording significant motion: Trans_Z+ ===\n",
      "Saved Trans_Z+_motion.gif\n",
      "\n",
      "=== Recording significant motion: Rot_X+ ===\n",
      "Saved Rot_X+_motion.gif\n",
      "\n",
      "=== Recording significant motion: Rot_Y+ ===\n",
      "Saved Rot_Y+_motion.gif\n",
      "\n",
      "=== Recording significant motion: Rot_Z+ ===\n",
      "Saved Rot_Z+_motion.gif\n"
     ]
    }
   ],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "# ------------------- PHYSICS SETUP -------------------\n",
    "physicsClient = p.connect(p.GUI)\n",
    "time_step = 0.001\n",
    "p.resetSimulation()\n",
    "p.setTimeStep(time_step)\n",
    "p.setGravity(0, 0, -9.8)\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "\n",
    "# Load environment\n",
    "p.loadURDF('plane.urdf')\n",
    "\n",
    "# Place red box (target object)\n",
    "box_length = box_width = 0.4\n",
    "box_depth = 0.02\n",
    "object_center = [0.3, 1, 0.01]\n",
    "object_orientation = [0, 0, 0]\n",
    "object_color = [0.8, 0.0, 0.0, 1]\n",
    "geomBox = p.createCollisionShape(p.GEOM_BOX, halfExtents=[box_length/2, box_width/2, box_depth/2])\n",
    "visualBox = p.createVisualShape(p.GEOM_BOX, halfExtents=[box_length/2, box_width/2, box_depth/2],\n",
    "                                rgbaColor=object_color)\n",
    "boxId = p.createMultiBody(\n",
    "    baseMass=0,\n",
    "    baseCollisionShapeIndex=geomBox,\n",
    "    baseVisualShapeIndex=visualBox,\n",
    "    basePosition=np.array(object_center),\n",
    "    baseOrientation=p.getQuaternionFromEuler(object_orientation)\n",
    ")\n",
    "\n",
    "# Camera visualization setup\n",
    "p.resetDebugVisualizerCamera(cameraDistance=1, cameraYaw=30, cameraPitch=-52, cameraTargetPosition=[0, 0, 0.5])\n",
    "\n",
    "# ------------------- CAMERA PARAMETERS -------------------\n",
    "camera_width = 512\n",
    "camera_height = 512\n",
    "camera_focal_depth = 1.0\n",
    "motion_distance = 0.1       # meters for significant motion\n",
    "rotation_angle = np.deg2rad(15)  # radians\n",
    "num_steps = 40              # smooth frames per motion\n",
    "fps = 5\n",
    "\n",
    "# Initial camera pose\n",
    "camera_init_pos = np.array([0.2, 0, 1])\n",
    "camera_init_rot = np.array([[1, 0, 0],\n",
    "                            [0, -1, 0],\n",
    "                            [0, 0, -1]])\n",
    "\n",
    "# Define motions (6 DOFs)\n",
    "motions = {\n",
    "    \"No_Motion\": {\"delta_X\": np.array([0, 0, 0]), \"delta_Omega\": np.zeros(3)},\n",
    "    \"Trans_X+\": {\"delta_X\": np.array([motion_distance, 0, 0]), \"delta_Omega\": np.zeros(3)},\n",
    "    \"Trans_Y+\": {\"delta_X\": np.array([0, motion_distance, 0]), \"delta_Omega\": np.zeros(3)},\n",
    "    \"Trans_Z+\": {\"delta_X\": np.array([0, 0, motion_distance]), \"delta_Omega\": np.zeros(3)},\n",
    "    \"Rot_X+\":   {\"delta_X\": np.zeros(3), \"delta_Omega\": np.array([rotation_angle, 0, 0])},\n",
    "    \"Rot_Y+\":   {\"delta_X\": np.zeros(3), \"delta_Omega\": np.array([0, rotation_angle, 0])},\n",
    "    \"Rot_Z+\":   {\"delta_X\": np.zeros(3), \"delta_Omega\": np.array([0, 0, rotation_angle])},\n",
    "}\n",
    "\n",
    "# ------------------- RECORD EACH MOTION -------------------\n",
    "for label, motion in motions.items():\n",
    "    print(f\"\\n=== Recording significant motion: {label} ===\")\n",
    "    frames = []\n",
    "\n",
    "    # Reset camera to initial pose\n",
    "    cameraPosition = camera_init_pos.copy()\n",
    "    cameraOrientation = camera_init_rot.copy()\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Apply smooth motion\n",
    "        alpha = step / (num_steps - 1)\n",
    "        delta_X = motion[\"delta_X\"] * alpha\n",
    "        delta_Omega = motion[\"delta_Omega\"] * alpha\n",
    "\n",
    "        # Translation update\n",
    "        cameraPosition = camera_init_pos + delta_X\n",
    "\n",
    "        # Rotation update using small-angle approximation\n",
    "        omega_skew = np.array([\n",
    "            [0, -delta_Omega[2], delta_Omega[1]],\n",
    "            [delta_Omega[2], 0, -delta_Omega[0]],\n",
    "            [-delta_Omega[1], delta_Omega[0], 0]\n",
    "        ])\n",
    "        delta_R = np.eye(3) + omega_skew\n",
    "        cameraOrientation = camera_init_rot @ delta_R\n",
    "\n",
    "        # Re-orthonormalize rotation matrix\n",
    "        U, _, Vt = np.linalg.svd(cameraOrientation)\n",
    "        cameraOrientation = U @ Vt\n",
    "\n",
    "        # --- Get camera image ---\n",
    "        rgb, depth = get_camera_img_float(cameraPosition, cameraOrientation)\n",
    "\n",
    "        # --- Get object pixel coordinates ---\n",
    "        viewMat, projMat = get_camera_view_and_projection_opencv(cameraPosition, cameraOrientation)\n",
    "        object_loc = opengl_plot_world_to_pixelspace(object_center, viewMat, projMat,\n",
    "                                                     camera_width, camera_height)\n",
    "\n",
    "        # --- Draw object location for visualization ---\n",
    "        img_disp = (rgb * 255).astype(np.uint8)\n",
    "        u, v = int(object_loc[0]), int(object_loc[1])\n",
    "        if 0 <= u < camera_width and 0 <= v < camera_height:\n",
    "            cv2.circle(img_disp, (u, v), 8, (0, 255, 0), -1)\n",
    "        cv2.putText(img_disp, f\"{label} step {step}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        frames.append(img_disp)\n",
    "        p.stepSimulation()\n",
    "\n",
    "    # Save motion GIF\n",
    "    gif_name = f\"{label}_motion.gif\"\n",
    "    imageio.mimsave(gif_name, frames, fps=fps)\n",
    "    print(f\"Saved {gif_name}\")\n",
    "\n",
    "p.disconnect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b55393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running motion test: Trans_X+ ===\n",
      "Saved: Trans_X+_motion.gif\n",
      "\n",
      "=== Running motion test: Trans_Y+ ===\n",
      "Saved: Trans_Y+_motion.gif\n",
      "\n",
      "=== Running motion test: Trans_Z+ ===\n",
      "Saved: Trans_Z+_motion.gif\n",
      "\n",
      "=== Running motion test: Rot_X+ ===\n",
      "Saved: Rot_X+_motion.gif\n",
      "\n",
      "=== Running motion test: Rot_Y+ ===\n",
      "Saved: Rot_Y+_motion.gif\n",
      "\n",
      "=== Running motion test: Rot_Z+ ===\n",
      "Saved: Rot_Z+_motion.gif\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "\n",
    "# Start the connection to the physics server\n",
    "physicsClient = p.connect(p.GUI)\n",
    "time_step = 0.001\n",
    "p.resetSimulation()\n",
    "p.setTimeStep(time_step)\n",
    "p.setGravity(0, 0, -9.8)\n",
    "\n",
    "# Set the path to the URDF files included with PyBullet\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "\n",
    "# load a plane URDF\n",
    "p.loadURDF('plane.urdf')\n",
    "\n",
    "# Place obstacles\n",
    "box_length = box_width = 0.4\n",
    "box_depth = 0.02\n",
    "object_center = [0.3, 1, 0.01]\n",
    "object_orientation = [0, 0, 0]\n",
    "object_color = [0.8, 0.0, 0.0, 1]\n",
    "geomBox = p.createCollisionShape(p.GEOM_BOX, halfExtents=[box_length/2, box_width/2, box_depth/2])\n",
    "visualBox = p.createVisualShape(p.GEOM_BOX, halfExtents=[box_length/2, box_width/2, box_depth/2], rgbaColor=object_color)\n",
    "boxId = p.createMultiBody(\n",
    "    baseMass=0,\n",
    "    baseCollisionShapeIndex=geomBox,\n",
    "    baseVisualShapeIndex=visualBox,\n",
    "    basePosition=np.array(object_center),\n",
    "    baseOrientation=p.getQuaternionFromEuler(object_orientation)\n",
    ")\n",
    "ObjectModelPos, modelOrn  = p.getBasePositionAndOrientation(boxId)\n",
    "\n",
    "#reset debug gui camera position so we can see the robot up close\n",
    "p.resetDebugVisualizerCamera( cameraDistance=1, cameraYaw=30, cameraPitch=-52, cameraTargetPosition=[0,0,.5])\n",
    "\n",
    "cameraPosition = np.array([0.2,0,1])\n",
    "cameraOrientation = np.array([[1,0,0],[0,-1,0],[0,0,-1]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Camera and image parameters\n",
    "camera_width = 512\n",
    "camera_height = 512\n",
    "camera_focal_depth = 1.0  # approximate focal length (adjust to your getImageJacobian convention)\n",
    "motion_delta = 0.02        # small translational delta (m)\n",
    "angle_delta = np.deg2rad(5)  # small angular delta (radians)\n",
    "num_steps = 100             # number of intermediate frames for smooth GIF\n",
    "fps = 5\n",
    "\n",
    "# Initial camera pose\n",
    "camera_init_pos = np.array([0.2, 0, 1])\n",
    "camera_init_rot = np.array([[1,0,0],[0,-1,0],[0,0,-1]])\n",
    "\n",
    "# 6-axis motion labels and corresponding deltas\n",
    "motions = {\n",
    "    \"Trans_X+\": {\"delta_X\": np.array([motion_delta, 0, 0]), \"delta_Omega\": np.zeros(3)},\n",
    "    \"Trans_Y+\": {\"delta_X\": np.array([0, motion_delta, 0]), \"delta_Omega\": np.zeros(3)},\n",
    "    \"Trans_Z+\": {\"delta_X\": np.array([0, 0, motion_delta]), \"delta_Omega\": np.zeros(3)},\n",
    "    \"Rot_X+\":   {\"delta_X\": np.zeros(3), \"delta_Omega\": np.array([angle_delta, 0, 0])},\n",
    "    \"Rot_Y+\":   {\"delta_X\": np.zeros(3), \"delta_Omega\": np.array([0, angle_delta, 0])},\n",
    "    \"Rot_Z+\":   {\"delta_X\": np.zeros(3), \"delta_Omega\": np.array([0, 0, angle_delta])},\n",
    "}\n",
    "\n",
    "for label, motion in motions.items():\n",
    "    print(f\"\\n=== Running motion test: {label} ===\")\n",
    "    frames = []\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Reset to initial pose each time to avoid drift\n",
    "        cameraPosition = camera_init_pos.copy()\n",
    "        cameraOrientation = camera_init_rot.copy()\n",
    "\n",
    "        # Apply proportional scaling of motion by step\n",
    "        delta_scale = (step / (num_steps - 1))\n",
    "        delta_X = motion[\"delta_X\"] * delta_scale\n",
    "        delta_Omega = motion[\"delta_Omega\"] * delta_scale\n",
    "\n",
    "        # Apply translation\n",
    "        cameraPosition = camera_init_pos + delta_X\n",
    "\n",
    "        # Apply rotation using small-angle approximation\n",
    "        omega_skew = np.array([[0, -delta_Omega[2], delta_Omega[1]],\n",
    "                               [delta_Omega[2], 0, -delta_Omega[0]],\n",
    "                               [-delta_Omega[1], delta_Omega[0], 0]])\n",
    "        delta_R = np.eye(3) + omega_skew\n",
    "        cameraOrientation = camera_init_rot @ delta_R\n",
    "        U, _, Vt = np.linalg.svd(cameraOrientation)\n",
    "        cameraOrientation = U @ Vt  # re-orthonormalize\n",
    "\n",
    "        # Render image\n",
    "        rgb, depth = get_camera_img_float(cameraPosition, cameraOrientation)\n",
    "\n",
    "        # Get pixel coordinates of the object\n",
    "        viewMat, projMat = get_camera_view_and_projection_opencv(cameraPosition, cameraOrientation)\n",
    "        object_loc = opengl_plot_world_to_pixelspace(object_center, viewMat, projMat,\n",
    "                                                     camera_width, camera_height)\n",
    "\n",
    "        # Draw object marker for visualization\n",
    "        img_disp = rgb.copy()\n",
    "        u, v = int(object_loc[0]), int(object_loc[1])\n",
    "        cv2.circle(img_disp, (u, v), 8, (0, 255, 0), -1)\n",
    "        cv2.putText(img_disp, f\"{label} step {step}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "        frames.append((img_disp * 255).astype(np.uint8))\n",
    "\n",
    "    # Save GIF\n",
    "    gif_name = f\"{label}_motion.gif\"\n",
    "    imageio.mimsave(gif_name, frames, fps=fps)\n",
    "    print(f\"Saved: {gif_name}\")\n",
    "\n",
    "p.disconnect()\n",
    "\n",
    "# for ITER in range(200):\n",
    "#     p.stepSimulation()\n",
    " \n",
    "#     ''' Get Image'''\n",
    "#     rgb, depth = get_camera_img_float(cameraPosition, cameraOrientation)\n",
    "        \n",
    "#     ''' Magical Computer Vision Algorithm that gets locations of objects in the image, as object_loc (Do Not Remove)'''\n",
    "#     viewMat, projMat = get_camera_view_and_projection_opencv(cameraPosition, cameraOrientation)\n",
    "#     object_loc = opengl_plot_world_to_pixelspace(object_center, viewMat, projMat,camera_width, camera_height)\n",
    "    \n",
    "#     ''' Get Image Jacobian '''\n",
    "#     imageJacobian = getImageJacobian(object_loc[0], object_loc[1], depth, camera_focal_depth, camera_width, camera_height)\n",
    "\n",
    "#     ''' Get Camera Control '''\n",
    "#     delta_X, delta_Omega = findCameraControl(object_location_desired, object_loc, imageJacobian)\n",
    "    \n",
    "#     ''' Apply Camera Controls'''\n",
    "#     # Update camera position using proportional control\n",
    "#     cameraPosition = cameraPosition + K_p_x * delta_X\n",
    "    \n",
    "#     # Update camera orientation using proportional control for angular velocity\n",
    "#     # Convert angular velocity to rotation matrix update\n",
    "#     # For small rotations, we can approximate: R_new ≈ R * (I + [omega]_×)\n",
    "#     omega_skew = np.array([[0, -delta_Omega[2], delta_Omega[1]],\n",
    "#                            [delta_Omega[2], 0, -delta_Omega[0]],\n",
    "#                            [-delta_Omega[1], delta_Omega[0], 0]])\n",
    "#     delta_R = np.eye(3) + K_p_Omega * omega_skew\n",
    "#     cameraOrientation = cameraOrientation @ delta_R\n",
    "#     # Re-orthonormalize to ensure it remains a rotation matrix\n",
    "#     U, _, Vt = np.linalg.svd(cameraOrientation)\n",
    "#     cameraOrientation = U @ Vt\n",
    "\n",
    "#     # show image\n",
    "#     cv2.imshow(\"depth\", depth)\n",
    "#     cv2.imshow(\"rgb\", rgb)\n",
    "#     cv2.waitKey(1)\n",
    "\n",
    "# #close the physics server\n",
    "# cv2.destroyAllWindows()    \n",
    "# p.disconnect() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11cb60e",
   "metadata": {},
   "source": [
    "### Q3-Q4 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f509e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create Robot Instance'''\n",
    "pandaUid = p.loadURDF(os.path.join(pybullet_data.getDataPath(), \"franka_panda\\\\panda.urdf\"),useFixedBase=True)\n",
    "p.resetBasePositionAndOrientation(pandaUid, [0, 0, 0], [0, 0, 0, 1])\n",
    "initialJointPosition = [0,-np.pi/4,np.pi/4,-np.pi/4,np.pi/4,np.pi/4,np.pi/4,0,0,0,0,0]\n",
    "robot = eye_in_hand_robot(pandaUid,initialJointPosition)\n",
    "p.stepSimulation() # need to do this to initialize robot\n",
    "\n",
    "for ITER in range(200):\n",
    "    p.stepSimulation()\n",
    "    '''Q3b'''\n",
    "    #object_center=[0.5+0.2*np.sin(np.pi/2+ITER/10), 0.5+0.2*np.sin(ITER/20), 0.01]\n",
    "    #p.resetBasePositionAndOrientation(boxId,object_center,p.getQuaternionFromEuler(object_orientation))\n",
    " \n",
    "    ''' Match Camera Pose to Robot End-Effector and Get Image'''\n",
    "    # Get end-effector position and orientation\n",
    "    cameraPosition, cameraOrientation = robot.get_ee_position()\n",
    "    \n",
    "    # Get image from camera at end-effector\n",
    "    rgb, depth = get_camera_img_float(cameraPosition, cameraOrientation)\n",
    "    \n",
    "    ''' Magical Computer Vision Algorithm that gets locations of objects in the image, as object_loc (Do Not Remove)'''\n",
    "    viewMat, projMat = get_camera_view_and_projection_opencv(cameraPosition, cameraOrientation)\n",
    "    object_loc = opengl_plot_world_to_pixelspace(object_center, viewMat, projMat,camera_width, camera_height)\n",
    "\n",
    "    ''' Do some things here to get robot control'''\n",
    "    # Get Image Jacobian\n",
    "    imageJacobian = getImageJacobian(object_loc[0], object_loc[1], depth, camera_focal_depth, camera_width, camera_height)\n",
    "    \n",
    "    # Get Camera Control (desired camera motion)\n",
    "    delta_X, delta_Omega = findCameraControl(object_location_desired, object_loc, imageJacobian)\n",
    "    \n",
    "    # Get Joint Control (convert camera motion to joint motion)\n",
    "    new_jointPositions = findJointControl(robot, delta_X, delta_Omega)\n",
    "    \n",
    "    #set Next Joint Targets\n",
    "    robot.set_joint_position(new_jointPositions)\n",
    "    \n",
    "    # show image\n",
    "    cv2.imshow(\"depth\", depth)\n",
    "    cv2.imshow(\"rgb\", rgb)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "#close the physics server\n",
    "cv2.destroyAllWindows()    \n",
    "p.disconnect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3251a775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b7de89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
